{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRGcUP04MXPQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/CSE425_VAE_Project\"\n",
        "if PROJECT_ROOT not in sys.path:\n",
        "    sys.path.append(PROJECT_ROOT)\n",
        "\n",
        "print(\"Added to sys.path:\", PROJECT_ROOT)\n"
      ],
      "metadata": {
        "id": "ZY121oHpWKGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA = \"/content/drive/MyDrive/CSE425_VAE_Project/data/raw/merge\"\n",
        "zip_path = f\"{DATA}/MERGE_Bimodal_Balanced.zip\"\n",
        "\n",
        "!unzip -o \"{zip_path}\" -d \"{DATA}\"\n"
      ],
      "metadata": {
        "id": "wt2MdMNzdC6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "ROOT = \"/content/drive/MyDrive/CSE425_VAE_Project/data/raw/merge/MERGE_Bimodal_Balanced\"\n",
        "OUT  = \"/content/drive/MyDrive/CSE425_VAE_Project/data/processed\"\n",
        "os.makedirs(OUT, exist_ok=True)\n",
        "\n",
        "rows = []\n",
        "\n",
        "audio_root = os.path.join(ROOT, \"audio\")\n",
        "lyrics_root = os.path.join(ROOT, \"lyrics\")\n",
        "\n",
        "for quadrant in [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]:\n",
        "    audio_dir = os.path.join(audio_root, quadrant)\n",
        "    lyrics_dir = os.path.join(lyrics_root, quadrant)\n",
        "\n",
        "    audio_files = {\n",
        "        os.path.splitext(f)[0]: os.path.join(audio_dir, f)\n",
        "        for f in os.listdir(audio_dir)\n",
        "        if f.lower().endswith((\".wav\", \".mp3\", \".flac\"))\n",
        "    }\n",
        "\n",
        "    lyrics_files = {\n",
        "        os.path.splitext(f)[0]: os.path.join(lyrics_dir, f)\n",
        "        for f in os.listdir(lyrics_dir)\n",
        "        if f.lower().endswith(\".txt\")\n",
        "    }\n",
        "\n",
        "    common_ids = sorted(set(audio_files) & set(lyrics_files))\n",
        "\n",
        "    for tid in common_ids:\n",
        "        rows.append({\n",
        "            \"track_id\": tid,\n",
        "            \"audio_path\": audio_files[tid],\n",
        "            \"lyrics_path\": lyrics_files[tid],\n",
        "            \"quadrant\": quadrant\n",
        "        })\n",
        "\n",
        "tracks = pd.DataFrame(rows)\n",
        "tracks.to_csv(os.path.join(OUT, \"tracks.csv\"), index=False)\n",
        "\n",
        "print(\"tracks.csv created\")\n",
        "print(\"Total samples:\", len(tracks))\n",
        "print(tracks.head())\n"
      ],
      "metadata": {
        "id": "RdlGC0VbTCtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.dataset import MergeAudioDataset\n",
        "\n",
        "ds = MergeAudioDataset(\"/content/drive/MyDrive/CSE425_VAE_Project/data/processed/tracks.csv\")\n",
        "x, q = ds[0]\n",
        "print(x.shape, x.min().item(), x.max().item(), q)\n"
      ],
      "metadata": {
        "id": "WL3T1gZdVuYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def cvae_loss(x, x_hat, mu, logvar, beta=4.0):\n",
        "    recon = F.mse_loss(x_hat, x, reduction=\"sum\")\n",
        "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return recon + beta * kl\n"
      ],
      "metadata": {
        "id": "jvOqlE5wWy72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, numpy as np, pandas as pd, librosa\n",
        "from tqdm import tqdm\n",
        "\n",
        "tracks_csv = \"/content/drive/MyDrive/CSE425_VAE_Project/data/processed/tracks.csv\"\n",
        "out_dir    = \"/content/drive/MyDrive/CSE425_VAE_Project/data/processed\"\n",
        "mels_path  = f\"{out_dir}/mels_1x128x128.npy\"\n",
        "q_path     = f\"{out_dir}/quadrants.npy\"\n",
        "\n",
        "df = pd.read_csv(tracks_csv)\n",
        "X = np.zeros((len(df), 1, 128, 128), dtype=np.float32)\n",
        "q_map = {\"Q1\":0, \"Q2\":1, \"Q3\":2, \"Q4\":3}\n",
        "Q = np.zeros((len(df),), dtype=np.int64)\n",
        "\n",
        "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    y, sr = librosa.load(row.audio_path, sr=22050, mono=True)\n",
        "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, n_fft=2048, hop_length=512)\n",
        "    mel = librosa.power_to_db(mel, ref=np.max)\n",
        "\n",
        "    mel = mel[:, :128]\n",
        "    if mel.shape[1] < 128:\n",
        "        mel = np.pad(mel, ((0,0),(0,128-mel.shape[1])), mode=\"constant\")\n",
        "\n",
        "    mel = (mel - mel.min()) / (mel.max() - mel.min() + 1e-8)\n",
        "    X[i, 0] = mel.astype(np.float32)\n",
        "    Q[i] = q_map[str(row.quadrant).strip()]\n",
        "\n",
        "np.save(mels_path, X)\n",
        "np.save(q_path, Q)\n",
        "\n",
        "print(\"Saved:\", mels_path, X.shape)\n",
        "print(\"Saved:\", q_path, Q.shape)\n"
      ],
      "metadata": {
        "id": "yzkhnppUjxxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class PrecomputedMelDataset(Dataset):\n",
        "    def __init__(self, mels_path, q_path):\n",
        "        self.X = np.load(mels_path)  # (N, 1, 128, 128)\n",
        "        self.Q = np.load(q_path)     # (N,)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.Q)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.from_numpy(self.X[idx]).float()     # (1,128,128)\n",
        "        q = torch.tensor(self.Q[idx], dtype=torch.long)\n",
        "        return x, q\n"
      ],
      "metadata": {
        "id": "UwPWJoJbj1OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = PrecomputedMelDataset(\n",
        "    \"/content/drive/MyDrive/CSE425_VAE_Project/data/processed/mels_1x128x128.npy\",\n",
        "    \"/content/drive/MyDrive/CSE425_VAE_Project/data/processed/quadrants.npy\"\n",
        ")\n",
        "x, q = dataset[0]\n",
        "print(x.shape, x.min().item(), x.max().item(), q)"
      ],
      "metadata": {
        "id": "ZYeOlN_hkzaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "6FiWGprgk-BU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the Encoder, Decoder, and CNN_CVAE classes\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1) # 128 -> 64\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1) # 64 -> 32\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1) # 32 -> 16\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1) # 16 -> 8\n",
        "        self.conv5 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1) # 8 -> 4\n",
        "        self.fc_mu = nn.Linear(512 * 4 * 4 + num_classes, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(512 * 4 * 4 + num_classes, latent_dim)\n",
        "\n",
        "    def forward(self, x, q):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = x.view(x.size(0), -1) # Flatten\n",
        "\n",
        "        # One-hot encode the quadrant and concatenate\n",
        "        q_onehot = F.one_hot(q, num_classes=4).float() # Assuming 4 quadrants\n",
        "        x = torch.cat([x, q_onehot], dim=1)\n",
        "\n",
        "        mu = self.fc_mu(x)\n",
        "        logvar = self.fc_logvar(x)\n",
        "        return mu, logvar\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(latent_dim + num_classes, 512 * 4 * 4)\n",
        "        self.deconv1 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1) # 4 -> 8\n",
        "        self.deconv2 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1) # 8 -> 16\n",
        "        self.deconv3 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1) # 16 -> 32\n",
        "        self.deconv4 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1) # 32 -> 64\n",
        "        self.deconv5 = nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1) # 64 -> 128\n",
        "\n",
        "    def forward(self, z, q):\n",
        "        q_onehot = F.one_hot(q, num_classes=4).float()\n",
        "        z = torch.cat([z, q_onehot], dim=1)\n",
        "\n",
        "        x = self.fc(z)\n",
        "        x = x.view(x.size(0), 512, 4, 4) # Reshape\n",
        "        x = F.relu(self.deconv1(x))\n",
        "        x = F.relu(self.deconv2(x))\n",
        "        x = F.relu(self.deconv3(x))\n",
        "        x = F.relu(self.deconv4(x))\n",
        "        x = torch.sigmoid(self.deconv5(x)) # Output a probability map (0-1)\n",
        "        return x\n",
        "\n",
        "class CNN_CVAE(nn.Module):\n",
        "    def __init__(self, latent_dim, num_classes=4): # Assuming 4 quadrants\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(latent_dim, num_classes)\n",
        "        self.decoder = Decoder(latent_dim, num_classes)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x, q):\n",
        "        mu, logvar = self.encoder(x, q)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_hat = self.decoder(z, q)\n",
        "        return x_hat, mu, logvar, z\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "dataset = PrecomputedMelDataset(\n",
        "    \"/content/drive/MyDrive/CSE425_VAE_Project/data/processed/mels_1x128x128.npy\",\n",
        "    \"/content/drive/MyDrive/CSE425_VAE_Project/data/processed/quadrants.npy\"\n",
        ")\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=128,          # larger batch now possible\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "model = CNN_CVAE(latent_dim=32).to(device)\n",
        "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(60):\n",
        "    total_loss = 0.0\n",
        "    for x, q in loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        q = q.to(device, non_blocking=True)\n",
        "\n",
        "        x_hat, mu, logvar, _ = model(x, q)\n",
        "        loss = cvae_loss(x, x_hat, mu, logvar, beta=4.0)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: loss = {total_loss/len(dataset):.4f}\")\n"
      ],
      "metadata": {
        "id": "A341E-GeW0i1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "model.eval()\n",
        "\n",
        "Z_list = []\n",
        "Q_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, q in tqdm(loader, desc=\"Extracting latents\"):\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        q = q.to(device, non_blocking=True)\n",
        "\n",
        "        _, mu, logvar, z = model(x, q)  # z is sampled\n",
        "        # For clustering, mu is usually more stable than sampled z\n",
        "        Z_list.append(mu.cpu().numpy())\n",
        "        Q_list.append(q.cpu().numpy())\n",
        "\n",
        "Z = np.vstack(Z_list)      # (N, latent_dim)\n",
        "Q = np.concatenate(Q_list) # (N,)\n",
        "\n",
        "print(\"Z shape:\", Z.shape, \"Q shape:\", Q.shape)\n"
      ],
      "metadata": {
        "id": "XzcTTTRdlOkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n"
      ],
      "metadata": {
        "id": "Kys_qJQd6A6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
        "\n",
        "kmeans = KMeans(n_clusters=4, random_state=42, n_init=20)\n",
        "C = kmeans.fit_predict(Z)\n",
        "\n",
        "sil = silhouette_score(Z, C)\n",
        "ari = adjusted_rand_score(Q, C)\n",
        "nmi = normalized_mutual_info_score(Q, C)\n",
        "\n",
        "print(\"KMeans Results\")\n",
        "print(\"Silhouette:\", sil)\n",
        "print(\"ARI:\", ari)\n",
        "print(\"NMI:\", nmi)\n"
      ],
      "metadata": {
        "id": "OO5HKFM8lSVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agglomerative**"
      ],
      "metadata": {
        "id": "y-3oKX0OPpO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
        "\n",
        "agg = AgglomerativeClustering(n_clusters=4, linkage=\"ward\")\n",
        "C_agg = agg.fit_predict(Zs)   # Zs = scaled features\n",
        "\n",
        "print(\"Agglomerative\")\n",
        "print(\"Silhouette:\", silhouette_score(Zs, C_agg))\n",
        "print(\"ARI:\", adjusted_rand_score(Qtrue, C_agg))\n"
      ],
      "metadata": {
        "id": "E0L3Jt9XPmc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DBSCAN**"
      ],
      "metadata": {
        "id": "Sa_wYD4CPuNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "\n",
        "db = DBSCAN(eps=1.5, min_samples=10)\n",
        "C_db = db.fit_predict(Zs)\n",
        "\n",
        "# Ignore noise points (-1) for silhouette if needed\n",
        "mask = C_db != -1\n",
        "print(\"DBSCAN Davies–Bouldin:\", davies_bouldin_score(Zs[mask], C_db[mask]))\n"
      ],
      "metadata": {
        "id": "3uZqEelKPtf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above DBSCAN will show error and the reason is shown below.\n",
        "DBSCAN was evaluated as a density-based clustering method; however, it classified all samples as noise across a wide range of hyperparameters. This suggests that the learned representations do not form dense, separable clusters, which is consistent with the continuous and overlapping nature of emotion representations."
      ],
      "metadata": {
        "id": "IQ3M5HncRbdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "unique, counts = np.unique(C_db, return_counts=True)\n",
        "print(\"DBSCAN label counts:\", dict(zip(unique, counts)))\n"
      ],
      "metadata": {
        "id": "irbk-kTjQthP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DB index"
      ],
      "metadata": {
        "id": "sTQLuZzyP5-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import davies_bouldin_score\n",
        "\n",
        "db_index = davies_bouldin_score(Zs, C)\n",
        "print(\"Davies–Bouldin Index:\", db_index)\n"
      ],
      "metadata": {
        "id": "7XxjMwlDP5MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "emb = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42).fit_transform(Z)\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(emb[:,0], emb[:,1], c=C, s=8)\n",
        "plt.title(\"UMAP of CVAE Latent Space (colored by KMeans cluster)\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(emb[:,0], emb[:,1], c=Q, s=8)\n",
        "plt.title(\"UMAP of CVAE Latent Space (colored by true Quadrant label)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "41IJ4NyFlYDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_dir = \"/content/drive/MyDrive/CSE425_VAE_Project/results\"\n",
        "import os\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "np.save(f\"{out_dir}/Z_mu.npy\", Z)\n",
        "np.save(f\"{out_dir}/Q_true.npy\", Q)\n",
        "np.save(f\"{out_dir}/clusters_kmeans.npy\", C)\n",
        "\n",
        "print(\"Saved latents and cluster labels to:\", out_dir)\n"
      ],
      "metadata": {
        "id": "NqRQeZv_lj3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n"
      ],
      "metadata": {
        "id": "85-hz1Bv18KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "Mu_list, Qemb_list, Q_list = [], [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, q in loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        q = q.to(device, non_blocking=True)\n",
        "\n",
        "        _, mu, _, _ = model(x, q)\n",
        "        # Create one-hot encoding for the quadrant, as model does not have a 'q_embed' attribute\n",
        "        q_onehot = F.one_hot(q, num_classes=4).float()\n",
        "\n",
        "        Mu_list.append(mu.cpu().numpy())\n",
        "        Qemb_list.append(q_onehot.cpu().numpy())\n",
        "        Q_list.append(q.cpu().numpy())\n",
        "\n",
        "Mu = np.vstack(Mu_list)\n",
        "Qemb = np.vstack(Qemb_list)\n",
        "Qtrue = np.concatenate(Q_list)\n",
        "\n",
        "Z_cond = np.concatenate([Mu, Qemb], axis=1)"
      ],
      "metadata": {
        "id": "MlrvZoW06FEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
        "import numpy as np\n",
        "\n",
        "def purity_score(y_true, y_pred):\n",
        "    # y_true and y_pred are 1D numpy arrays\n",
        "    clusters = np.unique(y_pred)\n",
        "    total = 0\n",
        "    for c in clusters:\n",
        "        idx = np.where(y_pred == c)[0]\n",
        "        if len(idx) == 0:\n",
        "            continue\n",
        "        labels, counts = np.unique(y_true[idx], return_counts=True)\n",
        "        total += counts.max()\n",
        "    return total / len(y_true)\n",
        "\n",
        "Zs = StandardScaler().fit_transform(Z_cond)\n",
        "\n",
        "kmeans = KMeans(n_clusters=4, n_init=50, random_state=42)\n",
        "C = kmeans.fit_predict(Zs)\n",
        "\n",
        "print(\"ARI:\", adjusted_rand_score(Qtrue, C))\n",
        "print(\"NMI:\", normalized_mutual_info_score(Qtrue, C))\n",
        "print(\"Purity:\", purity_score(Qtrue, C))"
      ],
      "metadata": {
        "id": "-u3PRrTL6lO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install sentence-transformers\n"
      ],
      "metadata": {
        "id": "u_oapuk7-X_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "tracks_csv = \"/content/drive/MyDrive/CSE425_VAE_Project/data/processed/tracks.csv\"\n",
        "out_dir = \"/content/drive/MyDrive/CSE425_VAE_Project/data/processed\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "lyrics_emb_path = f\"{out_dir}/lyrics_sbert.npy\"\n",
        "\n",
        "df = pd.read_csv(tracks_csv)\n",
        "\n",
        "def read_lyrics(path, max_chars=6000):\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            text = f.read().strip()\n",
        "        # keep it bounded (SBERT doesn't need super long)\n",
        "        return text[:max_chars] if text else \"\"\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "texts = [read_lyrics(p) for p in df[\"lyrics_path\"].tolist()]\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model_sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")  # fast + strong baseline\n",
        "\n",
        "emb = model_sbert.encode(\n",
        "    texts,\n",
        "    batch_size=64,\n",
        "    show_progress_bar=True,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True\n",
        ")\n",
        "\n",
        "np.save(lyrics_emb_path, emb)\n",
        "print(\"Saved SBERT lyrics embeddings:\", emb.shape, \"->\", lyrics_emb_path)\n"
      ],
      "metadata": {
        "id": "TD9dcy5Q-bq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "lyrics_emb = np.load(lyrics_emb_path)  # (N,384)\n",
        "\n",
        "pca = PCA(n_components=32, random_state=42)\n",
        "lyrics_32 = pca.fit_transform(lyrics_emb)\n",
        "\n",
        "print(\"Lyrics PCA shape:\", lyrics_32.shape)\n",
        "print(\"Explained variance (sum):\", pca.explained_variance_ratio_.sum())\n"
      ],
      "metadata": {
        "id": "HYCjCrReA6MT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "model.eval()\n",
        "\n",
        "Mu_list = []\n",
        "Q_list  = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, q in tqdm(loader, desc=\"Extracting audio mu\"):\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        q = q.to(device, non_blocking=True)\n",
        "        _, mu, _, _ = model(x, q)\n",
        "        Mu_list.append(mu.cpu().numpy())\n",
        "        Q_list.append(q.cpu().numpy())\n",
        "\n",
        "Mu_audio = np.vstack(Mu_list)      # (N,32)\n",
        "Qtrue    = np.concatenate(Q_list)  # (N,)\n",
        "\n",
        "print(\"Mu_audio:\", Mu_audio.shape, \"Qtrue:\", Qtrue.shape)\n"
      ],
      "metadata": {
        "id": "k3jilT_HA96v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Z_fused = np.concatenate([Mu_audio, lyrics_32], axis=1)\n",
        "print(\"Z_fused shape:\", Z_fused.shape)"
      ],
      "metadata": {
        "id": "jPEt9EpaBCL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
        "import numpy as np\n",
        "\n",
        "def purity_score(y_true, y_pred):\n",
        "    clusters = np.unique(y_pred)\n",
        "    total = 0\n",
        "    for c in clusters:\n",
        "        idx = np.where(y_pred == c)[0]\n",
        "        labels, counts = np.unique(y_true[idx], return_counts=True)\n",
        "        total += counts.max()\n",
        "    return total / len(y_true)\n",
        "\n",
        "Z_fused = np.concatenate([Mu_audio, lyrics_32], axis=1)  # (N,64)\n",
        "Zs = StandardScaler().fit_transform(Z_fused)\n",
        "\n",
        "kmeans = KMeans(n_clusters=4, n_init=50, random_state=42)\n",
        "C = kmeans.fit_predict(Zs)\n",
        "\n",
        "print(\"FUSED (Audio CVAE mu + Lyrics SBERT PCA) Results\")\n",
        "print(\"Silhouette:\", silhouette_score(Zs, C))\n",
        "print(\"ARI:\", adjusted_rand_score(Qtrue, C))\n",
        "print(\"NMI:\", normalized_mutual_info_score(Qtrue, C))\n",
        "print(\"Purity:\", purity_score(Qtrue, C))\n"
      ],
      "metadata": {
        "id": "jMBPInjyBNVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "emb2d = umap.UMAP(n_neighbors=20, min_dist=0.1, random_state=42).fit_transform(Zs)\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(emb2d[:,0], emb2d[:,1], c=C, s=8)\n",
        "plt.title(\"UMAP of Fused Representation (colored by KMeans clusters)\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(emb2d[:,0], emb2d[:,1], c=Qtrue, s=8)\n",
        "plt.title(\"UMAP of Fused Representation (colored by true Quadrant)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "D1eP61QnBW7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PCA+K-means**"
      ],
      "metadata": {
        "id": "UTQ6gJ8JSSv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load precomputed mels\n",
        "X = np.load(\"/content/drive/MyDrive/CSE425_VAE_Project/data/processed/mels_1x128x128.npy\")\n",
        "Qtrue = np.load(\"/content/drive/MyDrive/CSE425_VAE_Project/data/processed/quadrants.npy\")\n",
        "\n",
        "# Flatten spectrograms: (N, 1, 128, 128) → (N, 16384)\n",
        "X_flat = X.reshape(len(X), -1)\n",
        "print(\"Flattened shape:\", X_flat.shape)\n"
      ],
      "metadata": {
        "id": "buW9fNoqSPhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X_scaled = StandardScaler().fit_transform(X_flat)\n",
        "\n",
        "pca = PCA(n_components=32, random_state=42)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(\"PCA shape:\", X_pca.shape)\n",
        "print(\"Explained variance:\", pca.explained_variance_ratio_.sum())\n"
      ],
      "metadata": {
        "id": "fchqRnBUSdzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
        "\n",
        "def purity_score(y_true, y_pred):\n",
        "    total = 0\n",
        "    for c in np.unique(y_pred):\n",
        "        idx = np.where(y_pred == c)[0]\n",
        "        labels, counts = np.unique(y_true[idx], return_counts=True)\n",
        "        total += counts.max()\n",
        "    return total / len(y_true)\n",
        "\n",
        "kmeans = KMeans(n_clusters=4, n_init=50, random_state=42)\n",
        "C_pca = kmeans.fit_predict(X_pca)\n",
        "\n",
        "print(\"PCA + KMeans Results\")\n",
        "print(\"Silhouette:\", silhouette_score(X_pca, C_pca))\n",
        "print(\"ARI:\", adjusted_rand_score(Qtrue, C_pca))\n",
        "print(\"NMI:\", normalized_mutual_info_score(Qtrue, C_pca))\n",
        "print(\"Purity:\", purity_score(Qtrue, C_pca))\n"
      ],
      "metadata": {
        "id": "dBsntm4eSfQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Autoencoder + K-Means** CNN based autoencoder code in src/autoencoder.py"
      ],
      "metadata": {
        "id": "fezElDwfTOSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.autoencoder import CNN_Autoencoder\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Use the SAME precomputed dataset\n",
        "class AE_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X):\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx]\n",
        "\n",
        "ds_ae = AE_Dataset(X)\n",
        "loader_ae = torch.utils.data.DataLoader(ds_ae, batch_size=128, shuffle=True)\n",
        "\n",
        "ae = CNN_Autoencoder(latent_dim=32).to(device)\n",
        "opt = optim.Adam(ae.parameters(), lr=1e-3)\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "for epoch in range(15):\n",
        "    total = 0\n",
        "    for x in loader_ae:\n",
        "        x = x.to(device)\n",
        "        x_hat, _ = ae(x)\n",
        "        loss = loss_fn(x_hat, x)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total += loss.item()\n",
        "\n",
        "    print(f\"[AE] Epoch {epoch+1}: loss = {total/len(ds_ae):.4f}\")\n"
      ],
      "metadata": {
        "id": "HVBDGpWaTNrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ae.eval()\n",
        "Z_ae = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x in loader_ae:\n",
        "        x = x.to(device)\n",
        "        _, z = ae(x)\n",
        "        Z_ae.append(z.cpu().numpy())\n",
        "\n",
        "Z_ae = np.vstack(Z_ae)\n"
      ],
      "metadata": {
        "id": "7gYW91fCTgik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "Zs_ae = StandardScaler().fit_transform(Z_ae)\n",
        "\n",
        "C_ae = KMeans(n_clusters=4, n_init=50, random_state=42).fit_predict(Zs_ae)\n",
        "\n",
        "print(\"Autoencoder + KMeans Results\")\n",
        "print(\"Silhouette:\", silhouette_score(Zs_ae, C_ae))\n",
        "print(\"ARI:\", adjusted_rand_score(Qtrue, C_ae))\n",
        "print(\"NMI:\", normalized_mutual_info_score(Qtrue, C_ae))\n",
        "print(\"Purity:\", purity_score(Qtrue, C_ae))\n"
      ],
      "metadata": {
        "id": "NHaINk_iTiPE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}